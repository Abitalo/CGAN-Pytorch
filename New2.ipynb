{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from CGAN_MNIST import NetG, NetD\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_PATH = '../data/MNIST/mnist.pkl.gz'\n",
    "BATCH_SIZE = config.BATCH_SIZE\n",
    "MAX_EPOCH = config.MAX_EPOCH\n",
    "latent_dim = config.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, n_noise=latent_dim):\n",
    "    \"\"\"\n",
    "        save sample 100 images\n",
    "    \"\"\"\n",
    "    img = np.zeros([280, 280])\n",
    "    for j in range(10):\n",
    "        c = torch.zeros([10, 10]).to(device)\n",
    "        c[:, j] = 1\n",
    "        z = torch.randn(10, n_noise).to(device)\n",
    "        \n",
    "        y_hat = G(torch.cat((z,c),dim=1)).view(10, 28, 28)\n",
    "        result = y_hat.cpu().data.numpy()\n",
    "        img[j*28:(j+1)*28] = np.concatenate([x for x in result], axis=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(DATA_PATH, 'rb') as mnist:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(mnist, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(torch.detach,\n",
    "                                                        map(torch.Tensor,\n",
    "                                                           (x_train, y_train, x_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train-0.5)/0.5\n",
    "x_valid = (x_valid-0.5)/0.5\n",
    "ds_train = TensorDataset(x_train, y_train)\n",
    "ds_valid = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    dataset = ds_train,              \n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers=8,\n",
    "    shuffle = True,       \n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_valid = DataLoader(\n",
    "    dataset = ds_valid,              \n",
    "    batch_size = BATCH_SIZE,         \n",
    "    shuffle = True,       \n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    netG, netD = NetG().to(device), NetD().to(device)\n",
    "    # Adam 修改一下Momentum的参数，0.5等于focus on最近的2次迭代，默认的beta1 = 0.9，focus on最近10次有点太长了。\n",
    "    optG = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    optD = torch.optim.Adam(netD.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    return netG, optG, netD, optD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(G, D, optG, optD, criterion, start_epoch = 0, max_epoch = MAX_EPOCH):\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        for i, (x_gt, y_gt) in enumerate(dl_train):\n",
    "            # 将x,y转好格式\n",
    "            x_gt = x_gt.to(device) # (1024,784)float32\n",
    "            y_gt = F.one_hot(y_gt.to(torch.long)).to(torch.float).to(device) # (1024,10) float32,因为输入要cat，与z保持同类型\n",
    "            \n",
    "            # 固定G，训练D\n",
    "#             G.eval()\n",
    "#             D.train()              \n",
    "            # 生成Gaussian噪声\n",
    "            z = torch.randn((len(y_gt), latent_dim)).to(device) # (1024,100) float32\n",
    "   \n",
    "            out_x = D(torch.cat((x_gt, y_gt), dim=1))\n",
    "            loss_x = criterion(out_x, real_labels)  \n",
    "        \n",
    "            x_z = G(torch.cat((z, y_gt), dim=1)) # (1024,784) float32\n",
    "            out_z = D(torch.cat((x_z.detach(), y_gt), dim=1)) # (1024,1) float32 detach防止误差传播到G\n",
    "            loss_z = criterion(out_z, fake_labels)\n",
    "            loss_D = loss_z + loss_x\n",
    "            \n",
    "            optD.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optD.step()\n",
    "            \n",
    "            # 固定G，训练D\n",
    "            G.train()\n",
    "            D.eval()\n",
    "            # 生成Gaussian噪声\n",
    "            z = torch.randn((len(y_gt), latent_dim)).to(device) # (1024,100) float32\n",
    "            x_z = G(torch.cat((z, y_gt), dim=1)) # (1024,784) float32\n",
    "            out_z = D(torch.cat((x_z, y_gt), dim=1)) # (1024,1) float32\n",
    "            loss_G = criterion(out_z, real_labels)\n",
    "            \n",
    "            optG.zero_grad()\n",
    "            loss_G.backward()\n",
    "            optG.step()\n",
    "        \n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            G.eval()\n",
    "            print('Epoch:[%d/%d], Loss_G = %f, Loss_D = %f.\\n' %(epoch+1, max_epoch, loss_G.item(), loss_D.item()))\n",
    "            img = get_sample_image(G)\n",
    "#             imsave('samples/{}_epoch{}.jpg'.format(MODEL_NAME, str(epoch+1).zfill(3)), img, cmap='gray')\n",
    "            cv2.imwrite('samples/%d.jpg' % (epoch+1), img*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=110, out_features=128, bias=True)\n",
      "Linear(in_features=128, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=1024, bias=True)\n",
      "Linear(in_features=1024, out_features=784, bias=True)\n",
      "Linear(in_features=794, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=1024, bias=True)\n",
      "Linear(in_features=1024, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "criterion = F.binary_cross_entropy #predict使用onehot过的标签作为输入，但是计算损失还是只考虑是否为数字\n",
    "G, optG, D, optD = model_init()\n",
    "real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[10/500], Loss_G = 1.337309, Loss_D = 0.723467.\n",
      "\n",
      "Epoch:[20/500], Loss_G = 2.706451, Loss_D = 1.151028.\n",
      "\n",
      "Epoch:[30/500], Loss_G = 2.192599, Loss_D = 0.692334.\n",
      "\n",
      "Epoch:[40/500], Loss_G = 2.291567, Loss_D = 0.420434.\n",
      "\n",
      "Epoch:[50/500], Loss_G = 2.378611, Loss_D = 0.413174.\n",
      "\n",
      "Epoch:[60/500], Loss_G = 4.038810, Loss_D = 0.592034.\n",
      "\n",
      "Epoch:[70/500], Loss_G = 2.713987, Loss_D = 0.563897.\n",
      "\n",
      "Epoch:[80/500], Loss_G = 2.529819, Loss_D = 0.603097.\n",
      "\n",
      "Epoch:[90/500], Loss_G = 1.953907, Loss_D = 0.608995.\n",
      "\n",
      "Epoch:[100/500], Loss_G = 2.584807, Loss_D = 0.695384.\n",
      "\n",
      "Epoch:[110/500], Loss_G = 2.466191, Loss_D = 0.716644.\n",
      "\n",
      "Epoch:[120/500], Loss_G = 3.073211, Loss_D = 1.051528.\n",
      "\n",
      "Epoch:[130/500], Loss_G = 1.256303, Loss_D = 0.854432.\n",
      "\n",
      "Epoch:[140/500], Loss_G = 1.942178, Loss_D = 0.756161.\n",
      "\n",
      "Epoch:[150/500], Loss_G = 2.227684, Loss_D = 0.854831.\n",
      "\n",
      "Epoch:[160/500], Loss_G = 1.244714, Loss_D = 0.815726.\n",
      "\n",
      "Epoch:[170/500], Loss_G = 1.105452, Loss_D = 0.899203.\n",
      "\n",
      "Epoch:[180/500], Loss_G = 1.027015, Loss_D = 0.960187.\n",
      "\n",
      "Epoch:[190/500], Loss_G = 1.214067, Loss_D = 0.841098.\n",
      "\n",
      "Epoch:[200/500], Loss_G = 2.430915, Loss_D = 0.862154.\n",
      "\n",
      "Epoch:[210/500], Loss_G = 2.189059, Loss_D = 0.823932.\n",
      "\n",
      "Epoch:[220/500], Loss_G = 1.682878, Loss_D = 0.773652.\n",
      "\n",
      "Epoch:[230/500], Loss_G = 1.156984, Loss_D = 0.829062.\n",
      "\n",
      "Epoch:[240/500], Loss_G = 1.349552, Loss_D = 0.776172.\n",
      "\n",
      "Epoch:[250/500], Loss_G = 1.514485, Loss_D = 0.810191.\n",
      "\n",
      "Epoch:[260/500], Loss_G = 1.919451, Loss_D = 0.802149.\n",
      "\n",
      "Epoch:[270/500], Loss_G = 1.137061, Loss_D = 0.875804.\n",
      "\n",
      "Epoch:[280/500], Loss_G = 1.542561, Loss_D = 0.769883.\n",
      "\n",
      "Epoch:[290/500], Loss_G = 1.026437, Loss_D = 0.849850.\n",
      "\n",
      "Epoch:[300/500], Loss_G = 1.227832, Loss_D = 0.869969.\n",
      "\n",
      "Epoch:[310/500], Loss_G = 1.893919, Loss_D = 0.762710.\n",
      "\n",
      "Epoch:[320/500], Loss_G = 1.414690, Loss_D = 0.877870.\n",
      "\n",
      "Epoch:[330/500], Loss_G = 1.866711, Loss_D = 0.735677.\n",
      "\n",
      "Epoch:[340/500], Loss_G = 1.391537, Loss_D = 0.836460.\n",
      "\n",
      "Epoch:[350/500], Loss_G = 2.356081, Loss_D = 0.740882.\n",
      "\n",
      "Epoch:[360/500], Loss_G = 1.947003, Loss_D = 0.732429.\n",
      "\n",
      "Epoch:[370/500], Loss_G = 1.076569, Loss_D = 1.459366.\n",
      "\n",
      "Epoch:[380/500], Loss_G = 1.342655, Loss_D = 0.781772.\n",
      "\n",
      "Epoch:[390/500], Loss_G = 2.761672, Loss_D = 0.864972.\n",
      "\n",
      "Epoch:[400/500], Loss_G = 1.524882, Loss_D = 0.689429.\n",
      "\n",
      "Epoch:[410/500], Loss_G = 1.786209, Loss_D = 0.706267.\n",
      "\n",
      "Epoch:[420/500], Loss_G = 2.627874, Loss_D = 0.825050.\n",
      "\n",
      "Epoch:[430/500], Loss_G = 1.320294, Loss_D = 0.827750.\n",
      "\n",
      "Epoch:[440/500], Loss_G = 2.035906, Loss_D = 0.674424.\n",
      "\n",
      "Epoch:[450/500], Loss_G = 1.875184, Loss_D = 0.690887.\n",
      "\n",
      "Epoch:[460/500], Loss_G = 1.688439, Loss_D = 0.737751.\n",
      "\n",
      "Epoch:[470/500], Loss_G = 1.835464, Loss_D = 0.738837.\n",
      "\n",
      "Epoch:[480/500], Loss_G = 1.802822, Loss_D = 0.686704.\n",
      "\n",
      "Epoch:[490/500], Loss_G = 2.849084, Loss_D = 0.684754.\n",
      "\n",
      "Epoch:[500/500], Loss_G = 2.770939, Loss_D = 0.684271.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit(G, D, optG, optD, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_sample_image(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(DATA_PATH, 'rb') as mnist:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(mnist, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(torch.detach,\n",
    "                                                        map(torch.Tensor,\n",
    "                                                           (x_train, y_train, x_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1992, 0.6211, 0.9883, 0.6211, 0.1953, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1875, 0.9297, 0.9844, 0.9844, 0.9844, 0.9258, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2109, 0.8867, 0.9883, 0.9844, 0.9336, 0.9102, 0.9844, 0.2227,\n",
       "        0.0234, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0391, 0.2344, 0.8750, 0.9844, 0.9883, 0.9844, 0.7891, 0.3281, 0.9844,\n",
       "        0.9883, 0.4766, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.6367, 0.9844, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844, 0.3750,\n",
       "        0.7383, 0.9883, 0.6523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1992, 0.9297, 0.9883, 0.9883, 0.7422, 0.4453, 0.9883, 0.8906,\n",
       "        0.1836, 0.3086, 0.9961, 0.6562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1875, 0.9297, 0.9844, 0.9844, 0.6992, 0.0469, 0.2930, 0.4727,\n",
       "        0.0820, 0.0000, 0.0000, 0.9883, 0.9492, 0.1953, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1484, 0.6445, 0.9883, 0.9102, 0.8125, 0.3281, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.6445, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0273, 0.6953, 0.9844, 0.9375, 0.2773, 0.0742, 0.1094, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.7617, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2227, 0.9844, 0.9844, 0.2461, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.9844, 0.7617,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.7734, 0.9883, 0.7422, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9883,\n",
       "        0.7656, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2969, 0.9609, 0.9844, 0.4375, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883,\n",
       "        0.9844, 0.5781, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8984, 0.0977, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.5273,\n",
       "        0.9883, 0.7266, 0.0469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.8711, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0273, 0.5117,\n",
       "        0.9844, 0.8789, 0.2773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320, 0.9844, 0.5664,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1875, 0.6445,\n",
       "        0.9844, 0.6758, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3359, 0.9883,\n",
       "        0.8789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4453, 0.9297,\n",
       "        0.9883, 0.6328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3320,\n",
       "        0.9844, 0.9727, 0.5703, 0.1875, 0.1133, 0.3320, 0.6953, 0.8789, 0.9883,\n",
       "        0.8711, 0.6523, 0.2188, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.3320, 0.9844, 0.9844, 0.9844, 0.8945, 0.8398, 0.9844, 0.9844, 0.9844,\n",
       "        0.7656, 0.5078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1094, 0.7773, 0.9844, 0.9844, 0.9883, 0.9844, 0.9844, 0.9102,\n",
       "        0.5664, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0977, 0.5000, 0.9844, 0.9883, 0.9844, 0.5508,\n",
       "        0.1445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
